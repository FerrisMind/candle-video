# SVD Candle Integration â€” Analysis Report

**Date:** 2025-12-29  
**Status:** âœ… Analysis Complete  
**Next Phase:** Planning

---

## Executive Summary

Ð ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ **Stable Video Diffusion (SVD)** inference Ð² Rust/Candle Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ð´ÑƒÐ»Ñ `src/svd/` Ñ UNet-based SpatioTemporal Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¾Ð¹. Ð­Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð°ÐµÑ‚ÑÑ Ð¾Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ³Ð¾ DiT-based LTX-Video. Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ñ€ÐµÑ„ÐµÑ€ÐµÐ½Ñ â€” **diffusers**.

---

## 1. Scope v1 (Ð—Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÐºÑ‚)

| ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ | Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ | Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº |
|----------|----------|----------|
| **Ð ÐµÐ¶Ð¸Ð¼** | Image-to-Video (img2vid) | diffusers |
| **ÐšÐ°Ð´Ñ€Ñ‹** | 14 | unet.config.num_frames |
| **Ð Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ** | 576Ã—1024 | HF model card |
| **Dtype** | FP16 (Ñ force_upcast Ð´Ð»Ñ VAE) | diffusers pipeline |
| **Guidance** | `linspace(min, max, num_frames)` per-frame | pipeline |
| **fps conditioning** | `fps - 1` | micro-conditioning |
| **Ð’ÐµÑÐ°** | HF ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ (unet/, vae/, image_encoder/) | HF repo |

---

## 2. Architecture Overview

### 2.1 Pipeline Flow (diffusers reference)

```
Input Image (PIL/Tensor)
    â”‚
    â”œâ”€â–º CLIPImageProcessor.preprocess() â†’ [1, 3, 224, 224]
    â”‚
    â”œâ”€â–º CLIPVisionModelWithProjection.forward()
    â”‚       â””â”€â–º image_embeddings: [B, 1024]
    â”‚
    â”œâ”€â–º VideoProcessor.preprocess() â†’ [1, 3, 576, 1024]
    â”‚       â””â”€â–º + noise_aug_strength * noise
    â”‚
    â”œâ”€â–º VAE.encode() [force_upcast to FP32 if FP16]
    â”‚       â””â”€â–º image_latents: [B, 4, 72, 128]
    â”‚       â””â”€â–º repeat for num_frames â†’ [B, 14, 4, 72, 128]
    â”‚
    â”œâ”€â–º _get_add_time_ids(fps-1, motion_bucket_id, noise_aug_strength)
    â”‚       â””â”€â–º added_time_ids: [B, 3]
    â”‚
    â”œâ”€â–º prepare_latents() â†’ [B, 14, 4, 72, 128] random noise
    â”‚
    â”œâ”€â–º Denoising Loop (25 steps default):
    â”‚       â”‚
    â”‚       â”œâ”€â–º latent_model_input = cat([latents, latents]) for CFG
    â”‚       â”œâ”€â–º image_latents concat â†’ [B, 14, 8, 72, 128]
    â”‚       â”œâ”€â–º UNet.forward(sample, timestep, encoder_hidden_states, added_time_ids)
    â”‚       â”‚       â””â”€â–º noise_pred: [B, 14, 4, 72, 128]
    â”‚       â”œâ”€â–º CFG: noise = uncond + guidance_scale[frame] * (cond - uncond)
    â”‚       â””â”€â–º scheduler.step()
    â”‚
    â””â”€â–º VAE.decode(latents, num_frames=14)
            â””â”€â–º video: [B, 14, 3, 576, 1024]
```

### 2.2 UNet Architecture (UNetSpatioTemporalConditionModel)

```
in_channels: 8 (4 noise + 4 image latents concatenated)
out_channels: 4
block_out_channels: [320, 640, 1280, 1280]
cross_attention_dim: 1024 (CLIP embedding dim)
num_attention_heads: [5, 10, 20, 20]
num_frames: 14
addition_time_embed_dim: 256 (for fps, motion_bucket_id, noise_aug)
projection_class_embeddings_input_dim: 768

down_blocks:
  [0] CrossAttnDownBlockSpatioTemporal(320â†’320, heads=5, downsample=True)
  [1] CrossAttnDownBlockSpatioTemporal(320â†’640, heads=10, downsample=True)
  [2] CrossAttnDownBlockSpatioTemporal(640â†’1280, heads=20, downsample=True)
  [3] DownBlockSpatioTemporal(1280â†’1280, no cross-attn, downsample=False)

mid_block:
  UNetMidBlockSpatioTemporal(1280, heads=20)

up_blocks:
  [0] UpBlockSpatioTemporal(1280, no cross-attn)
  [1] CrossAttnUpBlockSpatioTemporal(1280â†’1280, heads=20)
  [2] CrossAttnUpBlockSpatioTemporal(1280â†’640, heads=10)
  [3] CrossAttnUpBlockSpatioTemporal(640â†’320, heads=5)
```

### 2.3 VAE Architecture (AutoencoderKLTemporalDecoder)

```
Encoder: Standard 2D (DownEncoderBlock2D)
  block_out_channels: [128, 256, 512, 512]
  latent_channels: 4
  scaling_factor: 0.18215

Decoder: TemporalDecoder
  MidBlockTemporalDecoder â†’ UpBlockTemporalDecoder Ã— 4
  + time_conv_out: Conv3d(3, 3, kernel=(3,1,1)) for frame blending
```

### 2.4 CLIP Image Encoder

```
CLIPVisionModelWithProjection:
  hidden_size: 1280
  image_size: 224
  patch_size: 14
  num_hidden_layers: 32
  num_attention_heads: 16
  projection_dim: 1024  â† Critical: need projection Linear layer
```

### 2.5 Scheduler (EulerDiscreteScheduler)

```
num_train_timesteps: 1000
beta_schedule: "scaled_linear"
beta_start: 0.00085
beta_end: 0.012
prediction_type: "v_prediction"  â† Important!
use_karras_sigmas: True
interpolation_type: "linear"
timestep_spacing: "leading"
steps_offset: 1
```

---

## 3. Key Implementation Decisions

### 3.1 Closed Ambiguities

| Question | Decision | Rationale |
|----------|----------|-----------|
| `image_only_indicator` | `zeros(batch, num_frames)` inside VAE decode | Matches diffusers, simplifies API |
| `force_upcast` | Implement: FP32 for VAE encode/decode when dtype=FP16 | Prevents NaN/artifacts |
| Weight naming | Explicit mapping layer with coverage test | Flexible, testable |
| Guidance per-frame | `linspace(min, max, num_frames)` | Required for correct dynamics |
| fps conditioning | Use `fps - 1` | Micro-conditioning from training |

### 3.2 Reusable Components

| Component | Source | Notes |
|-----------|--------|-------|
| `ClipVisionTransformer` | `candle-transformers` | Add projection Linear(1280 â†’ 1024) |
| `WeightLoader` | `src/loader.rs` | Full reuse with key mapping |
| `GroupNorm`, `LayerNorm` | `candle_nn` | Standard |
| `Conv2d`, `Linear` | `candle_nn` | Standard |

### 3.3 Components to Implement

| Component | Complexity | Dependencies |
|-----------|------------|--------------|
| `EulerDiscreteScheduler` | ðŸŸ¡ Medium | v_prediction, karras sigmas |
| `SpatioTemporalResBlock` | ðŸŸ¡ Medium | Temporal mixing, alpha blender |
| `TransformerSpatioTemporalModel` | ðŸ”´ High | Spatial + Temporal attention |
| `CrossAttnDownBlockSpatioTemporal` | ðŸŸ¡ Medium | ResBlock + Transformer |
| `UpBlockSpatioTemporal` | ðŸŸ¢ Low | ResBlock only |
| `CrossAttnUpBlockSpatioTemporal` | ðŸŸ¡ Medium | + Transformer |
| `UNetMidBlockSpatioTemporal` | ðŸŸ¡ Medium | Transformer + ResBlock |
| `UNetSpatioTemporalConditionModel` | ðŸ”´ High | All blocks + embeddings |
| `TemporalDecoder` | ðŸŸ¡ Medium | Temporal conv + upsampling |
| `AutoencoderKLTemporalDecoder` | ðŸŸ¡ Medium | 2D Encoder + TemporalDecoder |
| `CLIPVisionModelWithProjection` | ðŸŸ¢ Low | Wrapper + projection |
| `SVDPipeline` | ðŸ”´ High | All components |

---

## 4. File Structure

```
src/svd/
â”œâ”€â”€ mod.rs                    # Public API exports
â”œâ”€â”€ config.rs                 # SvdConfig, SvdUnetConfig, SvdVaeConfig, EulerSchedulerConfig
â”œâ”€â”€ scheduler.rs              # EulerDiscreteScheduler
â”œâ”€â”€ clip.rs                   # CLIPVisionModelWithProjection wrapper
â”œâ”€â”€ unet/
â”‚   â”œâ”€â”€ mod.rs                # UNetSpatioTemporalConditionModel
â”‚   â”œâ”€â”€ blocks.rs             # Down/Up/Mid SpatioTemporal blocks
â”‚   â”œâ”€â”€ resnet.rs             # SpatioTemporalResBlock
â”‚   â””â”€â”€ transformer.rs        # TransformerSpatioTemporalModel
â”œâ”€â”€ vae/
â”‚   â”œâ”€â”€ mod.rs                # AutoencoderKLTemporalDecoder
â”‚   â”œâ”€â”€ encoder.rs            # 2D Encoder (standard)
â”‚   â””â”€â”€ decoder.rs            # TemporalDecoder
â”œâ”€â”€ pipeline.rs               # SVDPipeline main entry
â””â”€â”€ weight_mapping.rs         # HF key â†’ Candle path mapping
```

---

## 5. Testing Strategy

| Stage | Metric | Threshold |
|-------|--------|-----------|
| **Scheduler** | timesteps/sigmas match | Exact |
| **CLIP** | Embedding shape & range | Â±1e-4 relative |
| **VAE encode** | Latent statistics | PSNR > 40dB |
| **VAE decode** | Frame quality | PSNR > 35dB |
| **UNet forward** | noise_pred error | < 1e-4 relative |
| **Full pipeline** | Visual quality | SSIM > 0.95 at fixed seed |

---

## 6. Weight Files (v1 Canonical)

```
models/svd/
â”œâ”€â”€ image_encoder/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ model.fp16.safetensors
â”œâ”€â”€ unet/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ diffusion_pytorch_model.fp16.safetensors
â”œâ”€â”€ vae/
â”‚   â””â”€â”€ config.json
â”‚   (VAE weights in unet safetensors or vae/diffusion_pytorch_model.safetensors)
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ scheduler_config.json
â””â”€â”€ model_index.json
```

---

## 7. Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Weight naming mismatch | ðŸŸ¡ Medium | ðŸ”´ High | Key mapping layer + test |
| FP16 numerical issues | ðŸŸ¡ Medium | ðŸ”´ High | force_upcast + monitoring |
| TransformerSpatioTemporal complexity | ðŸ”´ High | ðŸŸ¡ Medium | Incremental build + unit tests |
| UNet memory footprint | ðŸŸ¡ Medium | ðŸŸ¡ Medium | Gradient checkpointing if needed |

---

## 8. Engineering Principles Compliance

| Principle | Status | Notes |
|-----------|--------|-------|
| **YAGNI** | âœ… | v1 scope minimal: 14 frames, 576Ã—1024, no XT |
| **DRY** | âœ… | Reuse CLIP, loader; common attention abstraction possible |
| **SOLID** | âœ… | Isolated module, clear interfaces |
| **KISS** | âœ… | Direct diffusers port, no premature abstraction |

---

## 9. Next Steps (Planning Phase)

1. Create detailed task breakdown with estimates
2. Define dependency graph
3. Set up test fixtures (reference latents, embeddings)
4. Implement in order: Scheduler â†’ CLIP â†’ VAE â†’ UNet â†’ Pipeline

---

**Analysis Phase Complete. Ready for Planning.**
